%!TEX root = ../dissertation.tex

\chapter{Background}
\label{chapter:Background}

This chapter provides fundamentals of theoretical knowledge required to understand the work for the previously stated problem. Firstly, we will discuss about object detection and feature extraction with presentation of the state of the art solution used (section \ref{Object_detection}). Secondly, we will describe the visual servoing control theory and solutions (section \ref{Visual_Servoing}). Thirdly, we will explain what is a manipulator in terms of robotics and what is kinematics of robotic manipulators (section \ref{Robotic_manipulator}).

\section{Object Detection and Features Extractions}
\label{Object_detection}

Object detection is the fist step performed by the presented controller. This detection is used to classify and track the object in real-time when the arm is moving (since the camera is fixed to the end effector) but also if the object is moved since the controller is used in a dynamic environment. Feature extraction is the second step performed by the controller and is used to compute the visual servoing control law.

\subsection{Objection detection}

Objection detection is a classical problem in computer vision, and it can be stated as recognize what the objects are inside a given image and also where they are in the image. To achieve this task, this work use a state of the art object detection algorithm called \gls{YOLO} \cite{DBLP:journals/corr/RedmonDGF15}.
\gls{YOLO} is a convolutional neural network that was introduced in 2015 and recently improved its accuracy and speed on object detection with YOLOv2 \cite{DBLP:journals/corr/RedmonF16}. It is an object detection system targeted for real-time processing. 

\subsubsection{Functioning}

The first step is given an image, YOLO starts by splitting that image into an n\*n grid cells (figure : \ref{pict:yolo_grid}). In this grid, each of the cells (in red, in figure \ref{pict:yolo_grid}) is responsible for predicting n bounding box associate with a confidence score. 

\newpage

\begin{figure} [!ht]
    \centering
    \includegraphics[width=0.4\linewidth]{images/yolo_grid.png}
    \caption{Image splitted into a n*n grid, from YOLO website}
    \label{pict:yolo_grid}
\end{figure}

Each individual bounding box confidence score tells you how certain YOLO is that the predicted bounding box actually contains an object and also how well the bounding box feats the object.

\begin{figure} [!ht]
    \centering
    \includegraphics[width=0.4\linewidth]{images/yolo_confidence.png}
    \caption{Resulting prediction from all the grid cells (the thickness of the bounding box increase with the confidence, from YOLO website}
    \label{pict:yolo_confidence}
\end{figure}

In figure \ref{pict:yolo_confidence}, the prediction from all the cells regrouped together is visualized. This prediction take the aspect of many bounding box ranked by their confidence score. This way you know more or less where objects are in the image but you don't know what is the object.

The second step is for each cells (figure: \ref{pict:yolo_grid}) YOLO predicts a  class conditional probabilities. It only predicts one set of class probabilities per cell, regardless the number of bounding box associated with that cell. Note that since its using conditional probabilities, if a cell predict an object for example a bike it doesn't mean that inside of this cell there is this bike, it only means that if there is an object inside this bounding box then this object is a bike (figure \ref{pict:yolo_proba}).

\newpage

\begin{figure} [!ht]
    \centering
    \includegraphics[width=0.4\linewidth]{images/yolo_proba.png}
    \caption{ Class conditional probability map from YOLO website}
    \label{pict:yolo_proba}
\end{figure}

The third step is to combine both previous step (confidence score for the bounding box and class conditional probabilities) into one final score. This final score evaluates the confidence that each bounding box contains a specific object (figure \ref{pict:yolo_final}).

\begin{figure} [!ht]
    \centering
    \includegraphics[width=0.4\linewidth]{images/yolo_final.png}
    \caption{Bounding box and class conditional probabilities, from YOLO website}
    \label{pict:yolo_final}
\end{figure}

In figure \ref{pict:yolo_final}, their are still a lot of bounding box, the only remained step is to filter this bounding box with a final score that has to be higher than a specific threshold. Also, duplicated bounding boxes are removed using a non-maximum suppression. The final result is visible on figure \ref{pict:yolo_final_filter}.

\begin{figure} [!ht]
    \centering
    \includegraphics[width=0.3\linewidth]{images/yolo_final_filter.png}
    \caption{Final result after filtering, output from YOLO, from YOLO website}
    \label{pict:yolo_final_filter}
\end{figure}

\newpage

To summarize, each of the bounding boxes are composed by 5 predictions:
\begin{enumerate}
    \item x,y coordinates: offsets between the center of the bounding box relatives to the bounds of the cell.
    \item w,h: width and height of the bounding box
    \item confidence score, how YOLO is certain that this bounding box contains this object
\end{enumerate}

YOLO is a state of the art solution because it is a real time object detection system. What is the secret ? The output predictions are all computed at the same time looking only once the image (You Look Only Once). Since it predicts all of these detection simultaneously YOLO also implicitly incorporates global context in the detection process so it can learn things about which objects tend to occur together, the relative size and location of objects and other assorted things.

\subsubsection{Training}

YOLO is pre-trained on a ImageNet that is currently the biggest dataset for Computer Vision tasks (millions of images, more than a thousands class in different scenes). When pre-train, it learns to convert the millions images into a feature map (convolution weights). So when YOLO is actually trained with your own data set it adapts this convolutional weights to detect your own object class in the training set of images. Pre-train allows to have very good performance with a small training set. 

For the data augmentation, YOLO randomly scales and translates of up to 20\% of the original image size and also randomly adjusts the exposure and saturation of the image by up to a
factor of 1.5 and hue by up to a factor 0.1 in the HSV color space.

Overall the training of this network is straightforward and corresponds to a lot a of standards in the Computer Vision community: YOLO pre-train on ImageNet, uses stochastic gradient descent and uses data augmentation.

\begin{figure} [!ht]
    \centering
    \includegraphics[width=0.65\linewidth]{images/yolo_example.png}
    \caption{YOLO at work, street road example}
    \label{pict:yolo_example}
\end{figure}


\newpage

\subsection{Feature Extraction}

Feature extraction is the second step performed by the presented controller.

\subsubsection{Image feature}

Image feature is a simple image pattern, based on which what we see in the image can be described. The main role of features in computer visions is to transform visual information into the vector space. On the vector space we will be able to apply mathematical operations on this features.
This features can be for example points, lines or ellipses.
The selected feature as what his called a feature parameter, it is any numerical quantity associated to the selected feature in the image plane. This can be coordinates of a point, angular coefficient and offset of a line, center and radius of a circle, generalized moments of an object in the image.

\subsubsection{Image Formation}

In this section, the image acquisition process will be described. To this purpose we will use the perspective camera model. It is the model the most commonly used in computer vision, it reproduces with the behaviour of real cameras with the most accuracy \cite{Szeliski:2010:CVA:1941882}.

\begin{figure} [!ht]
    \centering
    \includegraphics[width=0.65\linewidth]{images/model.png}
    \caption{Ideal pinhole camera model}
    \label{pict:pinhole_camera_model}
\end{figure}

Given a 3D scene in the world, a camera projects one or more 3-D point(s) onto the 2D image plane. The main feature of the perspective projection is the conservation of straight lines, after the transformation. The image formation can be described with a model called the pinhole camera model \cite{Hartley2004} (Figure: \ref{pict:pinhole_camera_model}).

Figure \ref{pict:pinhole_camera_model} shows the ideal representation of the pinhole camera model, considering a central projection of points in space, onto an image plan defined by $Z = f$ where $f$ is the focal length. The optical center $C$, located at the origin of this 3D coordinate system, represents the point in which all the 3D projection intersect. This principal axis is defined as the line perpendicular to the image plane, passing through the optical center. The principal axis intersects the image plane in the principal point $p$ \cite{Hartley2004}.

A point $X = (X,Y,Z)^T$ defined in the coordinate system of the camera, is mapped on the image, dividing the coordinates X and Y by the point depth Z, and multiplying by the focal length f as described in

\begin{equation}
    X = (X,Y,Z)^T = (f \frac{X}{Z},f \frac{Y}{Z})^T
    \label{eq_image_1}
\end{equation}
\newpage

Using homogeneous coordinates (\ref{eq_image_1}) can be written as:
\begin{equation}
   x = PX
\end{equation}
With $ x = (f \frac{X}{Z},f \frac{Y}{Z})^T $ and $ X = (X, Y, Z, 1)^T$ and the camera matrix P is composed by the matrix $K$ of intrinsic parameters and the extrinsic parameters matrix composed by the rotation matrix $R$ and the translation matrix $t$.
\begin{equation}
   P = K \big[ R|t \big]
\end{equation}
The matrix K of intrinsic parameters, also called calibration matrix can be represent as:
\[
K 
=
\begin{bmatrix}
    f_{x} & s & c_{x} \\
    0 & f_{y} & c_{y}\\
    0 & 0 & 1
\end{bmatrix}
\]
where $f_{x}$ and $f_{y}$ are independent focal length for the camera sensor x and y dimensions. $c_{x}$ and $c_{y}$ denotes the optical frame center expressed in pixel coordinates.

$X$ will be expressed in the camera coordinate frame but the points in 3D space are expressed in the world coordinate frame. The extrinsic parameters matrix (E)  represent the relation between this two spaces. It represents the external position and orientation of the camera in the 3D world and relates the camera frame to the world frame:
\[
E
=
\begin{bmatrix}
    R_{[3x3]} & t \\
    0_{[1x3]} & 1
\end{bmatrix}
\]
an homogeneous matrix composed by a rotation matrix (R) and a translation vector (t).\\

To resume, in this section we have been looking into general knowledge about image features and image formation. Object detection and feature extraction are the first step in the algorithm that will be describe in the next chapter.

\section{Visual Servoing}
\label{Visual_Servoing}

\gls{VS} relies on techniques from areas like image processing, computer vision, and
control theory. Here, we present a definition of VS, Image-Based Visual Servoing (IBVS), \gls{PBVS}, and Hybrid method. Different ways of classifying these techniques are possible. Several classifications are extensively discussed in \cite{Staniak:2010:SVS:1831760.1831963}.
VS is the use of the vision to extract features and to process images to control robot pose. The introduction to visual feedback makes the system less sensitive to errors that could be generated by inadequate calibration, noise in the system or changes in the environment. Contrarily to open-loop approaches, like "looking" then "moving" methods, the use of visual information acquired in real time, makes the control scheme more robust and adaptive. VS, using visual-feedback control loop, increases the overall accuracy of the systems and appears as an attractive technique to achieve manipulation tasks.
Based on \cite{<inria-00350283>}, we will describe the various basic approaches in \gls{VS}. 
\newpage

\subsection{Image Base Visual Servoing}

IBVS uses measurements in the image m(t) (pixel coordinates, lines, moments
of region,...) converted to features using camera intrinsic parameters
p. The error signal is measured directly from the image. The control law is
based on the computed error between the set of current visual features and the set of 
desired features given by $s^*$. The interaction matrix relates a 2D point projected in
the image from a 3D point in the camera frame, by the depth of the point in
the camera frame
The error is defined as:
\begin{equation}
e(t)=s(m(t),p) - s^*
\end{equation}
with m(t), the computation from the image measurements, e.g. image coordinates of interest point, area, center of gravity.
p, other parameters like camera intrinsic parameters, 3D models of objects or the pose between camera and environment.

The time variation of $s$, $\Dot{s}$ and the camera spacial velocity, $v_{c}$ has been related by Chaumette in \cite{<inria-00350283>} like:

\begin{equation}
    \Dot{s} = L_{s}v_{c}
    \label{equation_1_ibvs}
\end{equation}
with $v_{c} = (v_{c}, w_{c})$, describing linear velocities and angular velocities relative to the origin of the camera frame.
$L_{s}$, is  called the interaction matrix (k, number of visual features * N, number of robot DoF).

The time variation of the error is equal to the time variation of the current features:
\[
\Dot{e} = \Dot{s}
\]
From \ref{equation_1_ibvs}, we can relate the time variation of the error with the precedent relation:
\begin{equation}
     \Dot{e} = L_{s}v_{c}
     \label{equation_2_ibvs}
\end{equation}
The goal is to reduce the error, to make the error converge to a lower value, to achieve this purpose an exponential decay was introduced \cite{<inria-00350283>} :
\begin{equation}
     \Dot{e} = - \lambda e
\end{equation}
From \ref{equation_2_ibvs} and since the control is done on the camera velocities the control law becomes:
\begin{equation}
    v_{c}= - \lambda L_{s}^+ e
\end{equation}
with $L_{s}^+$ the Moore-Penrose pseudo inverse matrix, that is defined as:
\[
L_{s}^+ = (L_{s}^T L_{s})^{-1} L_{s}^T
\]
when $(L_{s}^T L_{s}$ is of full rank. Both $L_{s}^T$ and $L_{s}$ are hard to be exactly computed in real \gls{VS} systems. They have to be estimated as $\hat{L_{s}}$ in the control law. 
\newpage
From a 3D point coordinates (X,Y,Z) and related to the image formation subsection, the point coordinates are defined in the image plan as:
\begin{equation}
    \begin{aligned}
    x = X / Z \\
    y = Y / Z
    \end{aligned}
\end{equation}
From this point coordinates, the current features and desired features are constructed: 
\begin{equation}
    \begin{aligned}
     s = (x, y)\\
     s^* = (x, y) 
    \end{aligned}
\end{equation}
Now, the time variation of the previous point coordinates can be written as:
\begin{equation}
\label{eq1_interaction}
    \begin{aligned}
    \Dot{x} = (\Dot{X} - x\Dot{Z})/Z \\
    \Dot{y} = (\Dot{Y} - y\Dot{Z})/Z
    \end{aligned}
\end{equation}
Relating the velocity of the 3D point with the camera spatial velocity is given by:
 \begin{equation}
 \label{eq3_interaction}
    \Dot{X} = - v_c - w_c * X
 \end{equation}
Becomes:
\begin{equation}
\label{eq2_interaction}
    \begin{aligned}
    \Dot{X} = - v_x - w_y * Z + w_z*Y \\
    \Dot{Y} = - v_x - w_y * Z + w_z*Y \\
    \Dot{Z} = - v_x - w_y * Z + w_z*Y
\end{aligned}
\end{equation}
Now using (\ref{eq2_interaction}) inside (\ref{eq1_interaction}):
\begin{equation}
\label{eq3_interaction}
    \begin{aligned}
    \Dot{x} = - \frac{v_x}{Z} -(1+x^2)w_y + w_z y + \frac{x}{Z} v_z + x y w_x \\
    \Dot{y} =- \frac{v_y}{Z} + (1+x^2)w_x + w_z x + \frac{x}{Z} v_z - x y w_y \\
\end{aligned}
\end{equation}
\ref{eq3_interaction} can be written as:
\begin{equation}
    \Dot{x} = L_x v_c
\end{equation}
Finally, the interaction matrix related to the 3D point is:
\[
\begin{bmatrix}
    - \frac{1}{Z} & 0 & \frac{x}{Z} & x y & -(1+x^2) & y \\
    0 & - \frac{1}{Z} & \frac{y}{Z} & 1+y^2 & - x y & x 
\end{bmatrix}
\]
If more than one point is used, the global interaction matrix will be stacking intermediate interaction matrix. For example if 3 points are used $x = (x_1, x_2, x_3)$:
\[
L_x
=
\begin{bmatrix}
    L_{x1} \\
    L_{x2} \\
    L_{x3} 
\end{bmatrix}
\]
\newpage

This matrix has to be estimated when used in control law and the quantity of points chosen has to be enough to avoid singular configurations and local minima.
They are several methods to estimate the interaction matrix in IBVS \cite{<inria-00350283>}. 

The most straight forward method is to estimate the current depth Z of each point, at each iteration of the control scheme:
\[
\Hat{L_s^+} = L_s^+ 
\]
According to \cite{<inria-00350283>}, this method gives a good local behavior but the global behavior is unsatisfactory. It means that the convergence will not be as good as expected.

Another approach is to set only the desired depth $Z^*$ of each point for the desired pose:
\[
\Hat{L_s^+} = L_{s^*}^+ 
\]
According to \cite{<inria-00350283>}, this method gives a good local behavior but introduce unexpected translational motion. 

A minimization approach as lately been proposed \cite{1308092} leading to chose:
\[
\Hat{L_s^+} = \frac{1}{2}(L_{s^*}^+ + L_s^+)
\]
This method gives a better trajectory with less oscillations. \\

Problems from using a \gls{IBVS} are referred in literature, like problems of singularities in the interaction matrix, or the reach of local minima can lead to problem in the control. It is also known as a highly coupled process. A correct estimation of the interaction matrix in IBVS is important to provide stability to the control system and avoid perturbations. 

\subsection{Pose Based Visual Servoing}



\section{Robotic manipulators and kinematics}
\label{Robotic_manipulator}


